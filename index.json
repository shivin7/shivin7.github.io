[{"authors":null,"categories":null,"content":"I am a graduate student, pursuing MS in Computational Linguistics at the University of Washington. My broad areas of interests include natural language processing, machine learning and data science.\nI graduated from BITS Pilani in Computer Science and Mathematics in 2020. Previously, I have had the opportunity to work at Avnio, Samsung RnD and, IMS Lab at University of Stuttgart.\nI am currently exploring internship and full-time opportunities starting Summer 2022 for the profiles of ML/NLP Engineer, Data Scientist and Software Engineer. If you find my profile relevant, feel free to leave a message here or contact me at one of my social handles.\n","date":1633046400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1633046400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a graduate student, pursuing MS in Computational Linguistics at the University of Washington. My broad areas of interests include natural language processing, machine learning and data science.\nI graduated from BITS Pilani in Computer Science and Mathematics in 2020.","tags":null,"title":"Shivin Thukral","type":"authors"},{"authors":["C.M. Downey","Shannon Drizin","Levon Haroutunian","Shivin Thukral"],"categories":null,"content":"","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633046400,"objectID":"cc2bfcb94328346adbd52bfae29eeea1","permalink":"https://shivin7.github.io/publication/xlslm/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/publication/xlslm/","section":"publication","summary":"We show that unsupervised sequence-segmentation performance can be transferred to extremely low-resource languages by pre-training a Masked Segmental Language Model (Downey et al., 2021) multilingually. Further, we show that this transfer can be achieved by training over a collection of low-resource languages that are typologically similar (but phylogenetically unrelated) to the target language. In our experiments, we transfer from a collection of 10 Indigenous American languages (AmericasNLP, Mager et al., 2021) to K'iche', a Mayan language. We compare our model to a monolingual baseline, and show that the multilingual pre-trained approach yields much more consistent segmentation quality across target dataset sizes, including a zero-shot performance of 20.6 F1, and exceeds the monolingual performance in 9/10 experimental settings. These results have promising implications for low-resource NLP pipelines involving human-like linguistic units, such as the sparse transcription framework proposed by Bird (2020).","tags":[],"title":"Multilingual Unsupervised Sequence Segmentation transfers to Extremely Low-resource Languages","type":"publication"},{"authors":["Shivin Thukral","Kunal Kukreja","Christian Kavouras"],"categories":null,"content":"","date":1630454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630454400,"objectID":"f93182ebad6a866a55ce8e2868698bfe","permalink":"https://shivin7.github.io/publication/temporal-exprs/","publishdate":"2021-09-01T00:00:00Z","relpermalink":"/publication/temporal-exprs/","section":"publication","summary":"We present three Natural Language Inference (NLI) challenge sets that can evaluate NLI models on their understanding of temporal expressions. More specifically, we probe these models for three temporal properties: (a) the order between points in time, (b) the duration between two points in time, (c) the relation between the magnitude of times specified in different units. We find that although large language models fine-tuned on MNLI have some basic perception of the order between points in time, at large, these models do not have a thorough understanding of the relation between temporal expressions.","tags":[],"title":"Probing Language Models for Understanding of Temporal Expressions","type":"publication"},{"authors":["Hemant Rathore","Sanjay K. Sahay","Shivin Thukral","Mohit Sewak"],"categories":null,"content":"","date":1612483200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612483200,"objectID":"f6ab69adb2cd1939d948b2e146448750","permalink":"https://shivin7.github.io/publication/android-paper/","publishdate":"2021-02-05T00:00:00Z","relpermalink":"/publication/android-paper/","section":"publication","summary":"Today anti-malware community is facing challenges due to ever-increasing sophistication and volume of malware attacks developed by adversaries. Traditional malware detection mechanisms are not able to cope-up against next-generation malware attacks. Therefore in this paper, we propose effective and efficient Android malware detection models based on machine learning and deep learning integrated with clustering. We performed a comprehensive study of different feature reduction, classification and clustering algorithms over various performance metrics to construct the Android malware detection models. Our experimental results show that malware detection models developed using Random Forest eclipsed deep neural network and other classifiers on the majority of performance metrics. The baseline Random Forest model without any feature reduction achieved the highest AUC of 99.4%. Also, the segregating of vector space using clustering integrated with Random Forest further boosted the AUC to 99.6% in one cluster and direct detection of Android malware in another cluster, thus reducing the curse of dimensionality. Additionally, we found that feature reduction in detection models does improve the model efficiency (training and testing time) many folds without much penalty on effectiveness of detection model.","tags":[],"title":"Detection of Malicious Android Applications: Classical Machine Learning vs. Deep Neural Network Integrated with Clustering","type":"publication"},{"authors":null,"categories":null,"content":"This project was done as an undergraduate thesis component, under the supervision of Prof. Sebastian Padó and Prof. Diego Frassinelli at Institute of Natural Language Processing, University of Stuttgart.\nAs part of this project:\n The motivation was to explore presence of cultural information in distributional vectors of different food terms Sentences for each food term were extracted from Wikipedia corpus and contextual embeddings were formed using BERT Framework Models were trained to predict norms of people (culture) from embedding space (language) which reached upto 0.7 correlation in few aspects Cross-lingual models over multiple norms performed better for English embeddings due to richer feature space  ","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"e5b821c536ffa317e8c1dfaf75d501e3","permalink":"https://shivin7.github.io/project/bits_thesis/","publishdate":"2019-12-01T00:00:00Z","relpermalink":"/project/bits_thesis/","section":"project","summary":"Undergraduate thesis completed at IMS Lab at University of Stuttgart","tags":["NLP"],"title":"Distributional Modelling of Cross-Cultural Judgement of Food Terms","type":"project"},{"authors":null,"categories":null,"content":"This was a design project under the supervision of Prof. Tirtharaj Dash at BITS Pilani, K K Birla Goa Campus. The aim of this project was to train a model which generates minimum noise to fool a pre-trained neural network.\nThe following was done as part of the design project :\n Initially, random Gaussian noise was added to different (deep/shallow) neural networks trained on MNIST data and their robustness was compared by measuring the minimum amount of noise that needed to be added to produce a certain threshold of misclassification After this we attempted to train the noise using a neural network. For this purpose, a basic dataset of modulus-2 was used. A classifier NN was trained on it, and then an adversarial NN was used to produce minimum noise which when added to inputs of the classifier NN produces wrong outputs. Two loss functions were used to train the adversarial NN :   Misclassification Loss – The accuracy between completely wrong labels of inputs and labels produced by classifier NN when inputs + noise by adversarial NN is fed Regularization Loss – L2 loss corresponding to the amount of noise produced by adversarial NN  After this, a Dog/Cat image dataset was used so that the noise added could be visualized easily, and it could be interpreted if the added noise is less perceptible to human eyes but crucial in changing the class of a pre-trained neural network. An auto-encoder architecture was employed for the same, and results were compared by trying different architectures, loss weightage, input size, etc. To try and improve the noise producing capacity of adversarial NN, we tried to convert it completely into a CNN, employing a deep architecture by using VGG16. However, we have had some problems in making the model converge to produce stable noise, and further work needs to be done by trying different combinations in architecture.  ","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554076800,"objectID":"5b3f5954855d646284804ff102d3ce83","permalink":"https://shivin7.github.io/project/robustness/","publishdate":"2019-04-01T00:00:00Z","relpermalink":"/project/robustness/","section":"project","summary":"Design project aimed at testing robustness of pre-trained classification models under adversarial noise attacks","tags":["Deep Learning"],"title":"Robustness Testing of Neural Networks","type":"project"},{"authors":null,"categories":null,"content":"This project was a part of the Information Retrieval course offered at BITS Pilani, K K Birla Goa Campus. The goal of this project is to perform sentiment analysis of Amazon Fine Food Reviews dataset, and this is done using an IR model in which each review is scored on a sentiment score of 0-5 against 5 different aspects:\n Taste Quality Product Quality Price Quantity Delivery  The main steps that go about into the IR model are:\n Sentence Decomposition   Forming micro-sentences from entire review [input] Exclamatory phrases like ‘Wow!’ form individual micro-sentences, and add to emotional weight of neighboring micro-sentences Isolated-word spelling correction using n-gram index edit distance Can use frameworks like NLTK for decomposition into micro-sentences  Feature-Emotion Extraction   List of features (example – food quality, service) composed Dictionary of emotions/adjectives along with their polarity score used (example – SenticNet) Dictionary of emojis/emoticons (text/image/encoded form) along with their polarity score used (example – Emoji Sentiment Ranking) POS-tagging used to extract object (feature) and adjective (emotion) of each micro-sentence Score of each feature calculated by corresponding emotion’s polarity score Polarity of emoji contributes to score of particular feature depending on context/position of emoji  Scoring   For each feature, scores of individual micro-sentences summed Final feature scores normalized to prevent discrimination between lenient/strict raters [output Final emotion (example – satisfied, angry) of user calculated on basis of weighted scores of all features [output]  ","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"3b3a53ae7d1c7e3081be98a3d7ee332d","permalink":"https://shivin7.github.io/project/emotion/","publishdate":"2018-12-01T00:00:00Z","relpermalink":"/project/emotion/","section":"project","summary":"Project as part of the 'Information Retrieval' course to perform sentiment analysis of food reviews","tags":["NLP"],"title":"Emotion Analysis of Food Reviews","type":"project"},{"authors":null,"categories":null,"content":"This project was used for development of a website for tracking item inventory as per the requirement of Department of Backstage, BITS Goa. The website has since been used during all major fests and events for proper inventory management.\n","date":1517443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517443200,"objectID":"1c2a5dd8f04c087f5316de50fdf179f0","permalink":"https://shivin7.github.io/project/website/","publishdate":"2018-02-01T00:00:00Z","relpermalink":"/project/website/","section":"project","summary":"Developed a website for tracking item inventory as per the requirement of Department of Backstage, BITS Goa","tags":["Other"],"title":"Inventory Website Development for University Auditorium","type":"project"},{"authors":null,"categories":null,"content":"This project was a part of the Neural Networks and Fuzzy Logic course offered at BITS Pilani, K K Birla Goa Campus. The goal of this project was to train a neural network to take as input rainy images and produce rain-free images as output.\nAs part of this project:\n Different models (auto-encoders/GANs) were trained to produce rain free images in a supervised setting TensorFlow framework used to implement deep networks for model training Considerable rain-free images were produced both on synthetic and natural rainy images without much perturbations  ","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512086400,"objectID":"a6c93198e3d07891768ca8c353f89a09","permalink":"https://shivin7.github.io/project/deraining/","publishdate":"2017-12-01T00:00:00Z","relpermalink":"/project/deraining/","section":"project","summary":"Project as part of the 'Neural Networks' course to perform de-raining of rainy images","tags":["Deep Learning"],"title":"Image De-Raining using Deconvolutional Networks","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://shivin7.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]